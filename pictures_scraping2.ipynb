{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AncoPetiteMer/obsidian/blob/main/pictures_scraping2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "!pip install --quiet -U langchain langchain-openai openai google-auth-oauthlib google-auth-httplib2 google-api-python-client python-dotenv beautifulsoup4 requests langchain_community"
      ],
      "metadata": {
        "collapsed": true,
        "id": "nFigI7zhxWo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"\n",
        "Extraction des informations\n",
        "1. Ville : Extraire le nom de la ville où se situe le bien.\n",
        "2. Code Postal (CP) : Récupérer le code postal associé à la ville.\n",
        "3. Transaction : Identifier s’il s’agit d’une location, d’une vente ou des deux.\n",
        "4. Type de bien : Préciser s’il s’agit d’un terrain, bureaux, local d’activités, entrepôt, etc.\n",
        "5. Surface du bien : Extraire la surface totale du bien en m².\n",
        "6. Surface du terrain : Extraire la surface du terrain s'il est mentionné.\n",
        "7. Surface divisibilité : Extraire la surface minimale divisible si elle est mentionnée.\n",
        "8. Prix : Si c’est une location, récupérer le loyer annuel ou mensuel (en multipliant par 12 si c’est le cas). Si c’est une vente, récupérer le prix de vente.\n",
        "9. Localisation et prestations : Extraire les informations liées à l’emplacement et aux caractéristiques techniques (accessibilité, équipements, etc.).\n",
        "10. Caractéristiques et équipements : Extraire les caractéristiques et équipements du bien immobilier à partir de la description.\n",
        "\n",
        "Génération des phrases et URL\n",
        "1. Phrase résumé des surfaces : Générer une phrase type comme « Terrain de 5000 m² divisible à partir de 100 m² » ou « Bureaux de 250 m² non divisibles ».\n",
        "2. Accroche commerciale : Créer une accroche sous le format Type de bien + type d’acquisition + surface en m² + ville + (numéro du département) (ex : \"Local d'activités 440 m² à louer à Lyon (69)\").\n",
        "3. Phrase résumé du prix : Rédiger une phrase pour résumer le prix, par exemple : \"Loyer mensuel de 70 €/m² HT/HC\". Si le prix n’est pas mentionné : \"Nous consulter pour plus d'informations.\"\n",
        "4. Reformulation des informations extraites : Formuler une description de 120 mots reprenant les informations collectées (description du bien, emplacement, surface, loyer/prix de vente, etc.).\n",
        "5. URL : Générer une URL sous le modèle /typed'acquisition-typedebien-surfacedubienm2-commune-n°département (ex : /location-local-activites-250m2-saint-pantaleon-de-larche-19).\n",
        "6. Titre SEO : Créer un titre SEO sous le modèle type acquisition + type de bien + surface totale en m² + ville (ex : \"Location local d'activités 1220 m² Lyon\").\n",
        "7. Phrase résumé : Générer une phrase incitative à la lecture, par exemple : \"Découvrez ce local d’activités idéalement situé à Lyon, offrant 440 m² divisibles et accessible aux entreprises.\"\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "cA4aoXodxRvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile offres_immos.py\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import requests\n",
        "import random\n",
        "from bs4 import BeautifulSoup\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain_openai import ChatOpenAI\n",
        "from google.colab import auth, drive\n",
        "from google.auth import default\n",
        "from googleapiclient.discovery import build\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "from requests.exceptions import RequestException\n",
        "import logging\n",
        "import base64\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "from urllib.parse import urljoin, urlparse\n",
        "\n",
        "# Configuration du logger\n",
        "logger = logging.getLogger('my_logger')\n",
        "logger.setLevel(logging.DEBUG)\n",
        "ch = logging.StreamHandler()\n",
        "ch.setLevel(logging.DEBUG)\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "ch.setFormatter(formatter)\n",
        "logger.addHandler(ch)\n",
        "\n",
        "# Authentification et création du client Drive (utilisé globalement pour la gestion des images)\n",
        "auth.authenticate_user()\n",
        "drive_service = build('drive', 'v3')\n",
        "\n",
        "# -------------------------------\n",
        "# IMAGE RELEVANCE UTILS\n",
        "# -------------------------------\n",
        "def image_to_base64(img_url):\n",
        "    \"\"\"\n",
        "    Download an image from the provided URL and convert it to a base64-encoded string.\n",
        "\n",
        "    Args:\n",
        "        img_url (str): The URL of the image to download.\n",
        "\n",
        "    Returns:\n",
        "        str or None: A base64-encoded string representing the image in JPEG format,\n",
        "                     or None if the image cannot be downloaded or converted.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(img_url)\n",
        "        response.raise_for_status()\n",
        "        image = Image.open(BytesIO(response.content))\n",
        "        buffered = BytesIO()\n",
        "        image.save(buffered, format=\"JPEG\")\n",
        "        return base64.b64encode(buffered.getvalue()).decode()\n",
        "    except Exception as e:\n",
        "        logger.error(\"Error converting image to base64 for URL %s: %s\", img_url, e)\n",
        "        return None\n",
        "\n",
        "# -------------------------------\n",
        "# Other utility functions\n",
        "# -------------------------------\n",
        "def scrapemyurl(url):\n",
        "    logger.info(\"Starting to scrape images from: %s\", url)\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    img_tags = soup.find_all('img')\n",
        "    logger.info(\"Found %s image tags.\", len(img_tags))\n",
        "    div_bg_tags = soup.find_all('div', attrs={'data-bg': True})\n",
        "    logger.info(\"Found %s div elements with data-bg attributes.\", len(div_bg_tags))\n",
        "    div_style_tags = soup.find_all('div', style=lambda value: value and 'background-image' in value)\n",
        "    logger.info(\"Found %s div elements with background-image in style attributes.\", len(div_style_tags))\n",
        "    return img_tags, div_bg_tags, div_style_tags\n",
        "\n",
        "def is_real_image(img_url):\n",
        "    non_real_keywords = ['favicon', 'icon', 'logo', 'sprite', 'placeholder']\n",
        "    for keyword in non_real_keywords:\n",
        "        if keyword in img_url.lower():\n",
        "            logger.debug(\"Image URL %s is not real because it contains the keyword '%s'.\", img_url, keyword)\n",
        "            return False\n",
        "    logger.debug(\"Image URL %s is considered real.\", img_url)\n",
        "    return True\n",
        "\n",
        "def has_sufficient_size(img_url):\n",
        "    try:\n",
        "        response = requests.get(img_url, stream=True)\n",
        "        response.raise_for_status()\n",
        "        image = Image.open(BytesIO(response.content))\n",
        "        width, height = image.size\n",
        "        if width > 100 and height > 100:\n",
        "            logger.debug(\"Image URL %s has sufficient size: %sx%s.\", img_url, width, height)\n",
        "            return True\n",
        "        else:\n",
        "            logger.debug(\"Image URL %s does not have sufficient size: %sx%s.\", img_url, width, height)\n",
        "            return False\n",
        "    except (requests.RequestException, IOError) as e:\n",
        "        logger.error(\"Failed to check size for image URL %s: %s\", img_url, e)\n",
        "        return False\n",
        "\n",
        "def is_image_content_type(img_url):\n",
        "    try:\n",
        "        response = requests.head(img_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "        content_type = response.headers.get('Content-Type', '')\n",
        "        if content_type.startswith('image/'):\n",
        "            logger.debug(\"Image URL %s has content type %s, which is an image.\", img_url, content_type)\n",
        "            return True\n",
        "        else:\n",
        "            logger.debug(\"Image URL %s has content type %s, which is not an image.\", img_url, content_type)\n",
        "            return False\n",
        "    except requests.RequestException:\n",
        "        logger.error(\"Failed to retrieve content type for image URL %s.\", img_url)\n",
        "        return False\n",
        "\n",
        "def extract_image_urls(tags, base_url):\n",
        "    image_urls = []\n",
        "    for tag in tags:\n",
        "        if tag.name == 'img':\n",
        "            img_url = tag.get('src') or tag.get('data-src')\n",
        "        elif tag.name == 'div':\n",
        "            img_url = tag.get('data-bg') or extract_background_image_url(tag.get('style'))\n",
        "        else:\n",
        "            continue\n",
        "        if img_url:\n",
        "            img_url = urljoin(base_url, img_url)\n",
        "            image_urls.append(img_url)\n",
        "            logger.debug(\"Extracted image URL: %s\", img_url)\n",
        "    return image_urls\n",
        "\n",
        "def extract_background_image_url(style):\n",
        "    if 'background-image' in style:\n",
        "        start = style.find('url(') + 4\n",
        "        end = style.find(')', start)\n",
        "        if start != -1 and end != -1:\n",
        "            img_url = style[start:end].strip().strip('\"').strip(\"'\")\n",
        "            logger.debug(\"Extracted background image URL from style: %s\", img_url)\n",
        "            return img_url\n",
        "    return None\n",
        "\n",
        "def save_image_to_drive(img_url, folder_id):\n",
        "    try:\n",
        "        response = requests.get(img_url, stream=True)\n",
        "        response.raise_for_status()\n",
        "        image = Image.open(BytesIO(response.content))\n",
        "        image.thumbnail((960, 720))\n",
        "        img_name = os.path.basename(img_url).split('.')[0] + '.jpeg'\n",
        "        img_path = f\"/tmp/{img_name}\"\n",
        "        image.convert(\"RGB\").save(img_path, 'JPEG', quality=90)\n",
        "        file_metadata = {'name': img_name, 'parents': [folder_id]}\n",
        "        media = MediaFileUpload(img_path, mimetype='image/jpeg')\n",
        "        file = drive_service.files().create(body=file_metadata, media_body=media, fields='id').execute()\n",
        "        file_id = file.get('id')\n",
        "        logger.info(\"Uploaded image %s to Google Drive with ID: %s\", img_url, file_id)\n",
        "    except requests.RequestException as e:\n",
        "        logger.error(\"Failed to save image %s to Google Drive: %s\", img_url, e)\n",
        "\n",
        "def create_folder_in_drive(folder_name, parent_folder_id):\n",
        "    file_metadata = {\n",
        "        'name': folder_name,\n",
        "        'mimeType': 'application/vnd.google-apps.folder',\n",
        "        'parents': [parent_folder_id]\n",
        "    }\n",
        "    file = drive_service.files().create(body=file_metadata, fields='id').execute()\n",
        "    folder_id = file.get('id')\n",
        "    logger.info(\"Created folder '%s' with ID: %s\", folder_name, folder_id)\n",
        "    return folder_id\n",
        "\n",
        "def get_domain_name(url):\n",
        "    parsed_url = urlparse(url)\n",
        "    domain_name = parsed_url.netloc\n",
        "    logger.debug(\"Extracted domain name: %s\", domain_name)\n",
        "    return domain_name\n",
        "\n",
        "def fetch_webpage_content(url, max_retries=3):\n",
        "    logger.info(\"Fetching content from URL: %s\", url)\n",
        "    user_agents = [\n",
        "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15',\n",
        "        'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:89.0) Gecko/20100101 Firefox/89.0',\n",
        "        'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36'\n",
        "    ]\n",
        "    session = requests.Session()\n",
        "    for i in range(max_retries):\n",
        "        try:\n",
        "            headers = {'User-Agent': random.choice(user_agents)}\n",
        "            time.sleep(2)\n",
        "            response = session.get(url, headers=headers)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            content = ''\n",
        "            main_content = soup.find('div', {'id': 'content'})\n",
        "            if main_content:\n",
        "                for tag in main_content.find_all(['h1', 'h2', 'h3', 'p', 'div', 'span']):\n",
        "                    content += tag.get_text(separator=' ', strip=True) + ' '\n",
        "            else:\n",
        "                for tag in ['h1', 'h2', 'h3', 'p', 'div', 'span']:\n",
        "                    elements = soup.find_all(tag)\n",
        "                    for element in elements:\n",
        "                        content += element.get_text(separator=' ', strip=True) + ' '\n",
        "            content = re.sub(r'\\s+', ' ', content)\n",
        "            content = re.sub(r'[\\r\\n]+', ' ', content)\n",
        "            content = re.sub(r'[^a-zA-Z0-9À-ÿ.,:;/\\-\\s]', '', content)\n",
        "            logger.info(\"Content fetched and parsed for URL: %s\", url)\n",
        "            return content.strip()\n",
        "        except RequestException as e:\n",
        "            logger.error(\"Error fetching URL %s: %s\", url, str(e))\n",
        "            if i == max_retries - 1:\n",
        "                return \"\"\n",
        "            wait_time = 2 ** i\n",
        "            logger.info(\"Retrying in %s seconds...\", wait_time)\n",
        "            time.sleep(wait_time)\n",
        "    return \"\"\n",
        "\n",
        "# Classe encapsulant les ressources partagées\n",
        "class OffresImmoProcessor:\n",
        "    \"\"\"\n",
        "    Classe responsable du traitement des offres immobilières.\n",
        "    Elle encapsule l'authentification, la configuration des services et le traitement des URL.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        logger.info(\"Mounting Google Drive\")\n",
        "        drive.mount('/content/drive')\n",
        "\n",
        "        logger.info(\"Loading API key from .env file\")\n",
        "        env_path = '/content/drive/MyDrive/Colab Notebooks/Api_keys/.env'\n",
        "        load_dotenv(env_path)\n",
        "        self.openai_api_key = os.getenv('OPENAI_API_KEY')\n",
        "        if not self.openai_api_key:\n",
        "            logger.error(\"Failed to load API key\")\n",
        "            raise Exception(\"API key not loaded\")\n",
        "        else:\n",
        "            logger.info(\"API key loaded successfully: %s...%s\", self.openai_api_key[:5], self.openai_api_key[-5:])\n",
        "\n",
        "        logger.info(\"Authenticating with Google\")\n",
        "        auth.authenticate_user()\n",
        "        creds, _ = default()\n",
        "        self.service = build('sheets', 'v4', credentials=creds)\n",
        "        logger.info(\"Google Sheets service created\")\n",
        "\n",
        "        logger.info(\"Setting up output parser and prompt template\")\n",
        "        self.response_schemas = [\n",
        "            ResponseSchema(name=\"ville\", description=\"La ville où se trouve le bien\"),\n",
        "            ResponseSchema(name=\"cp\", description=\"Le code postal, qui doit toujours être associé à la ville.\"),\n",
        "            ResponseSchema(name=\"transaction\", description=\"Type de transaction (location, vente, etc.)\"),\n",
        "            ResponseSchema(name=\"type_de_bien\", description=\"Type de bien immobilier à rentrer uniquement sous les déterminations suivantes (attention de bien respecter l'orthographe indiquée, ex : activite sans accent) : bureaux ou entrepot - logistique (pour tous les biens de type entrepot, batiment logistique, batiment industriel) ou local d'activite - atelier (incluant local commercial) ou terrain\"),\n",
        "            ResponseSchema(name=\"surface_du_bien\", description=\"Surface du bien en m²\"),\n",
        "            ResponseSchema(name=\"surface_du_terrain\", description=\"Surface du terrain en m²\"),\n",
        "            ResponseSchema(name=\"surface_divisibilite\", description=\"Surface divisible en m²\"),\n",
        "            ResponseSchema(name=\"prix_loyer_annuel\", description=\"Prix du loyer annuel\"),\n",
        "            ResponseSchema(name=\"prix_de_vente\", description=\"Prix de vente\"),\n",
        "            ResponseSchema(name=\"localisation\", description=\"Informations sur la localisation\"),\n",
        "            ResponseSchema(name=\"phrase_resume_surfaces\", description=\"Phrase résumant les surfaces\"),\n",
        "            ResponseSchema(name=\"accroche_commerciale\", description=\"Accroche commerciale générée sous le format Type de bien + type d’acquisition + surface en m² + ville + (numéro du département) ex : 'Local d'activités 440 m² à louer à Lyon (69)'\"),\n",
        "            ResponseSchema(name=\"phrase_resume_prix\", description=\"Phrase résumant le prix, par ex : 'Loyer mensuel de 70 €/m² HT/HC'. Si le prix n’est pas mentionné : 'Nous consulter pour plus d'informations.'\"),\n",
        "            ResponseSchema(name=\"reformulation\", description=\"Description reformulée de 120 mots, sans mentionner les prix, reprenant les informations collectées dont les équipements du site, les détails du bâtiment (revêtement de sol, matériaux extérieur, parking, portes sectionnelles, etc.) et les caractéristiques techniques (ex: accès poids lourds), sans commencer le paragraphe par 'découvrez', le ton doit être informatif\"),\n",
        "            ResponseSchema(name=\"url_modele\", description=\"URL générée sous le format '/typed'acquisition-typedebien-surfacedubienm2-commune-n°département' (ex : '/location-local-activites-250m2-saint-pantaleon-de-larche-19')\"),\n",
        "            ResponseSchema(name=\"titre_seo\", description=\"Titre SEO généré sous le format 'type acquisition + type de bien + ville (ex :'location local d'activités à Lyon') \"),\n",
        "            ResponseSchema(name=\"phrase_resume\", description=\"Phrase résumé incitative à la lecture, par exemple : 'Découvrez ce local d’activités idéalement situé à Lyon, offrant 440 m² divisibles et accessible aux entreprises.'\"),\n",
        "            ResponseSchema(name=\"caracteristiques_et_equipements\", description=\"Caractéristiques et équipements du bien immobilier extraits de la description\")\n",
        "        ]\n",
        "        output_parser = StructuredOutputParser.from_response_schemas(self.response_schemas)\n",
        "        logger.info(\"Creating ChatOpenAI model\")\n",
        "        self.llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0, api_key=self.openai_api_key)\n",
        "        logger.info(\"ChatOpenAI model created with model name: gpt-4o-mini\")\n",
        "        prompt_template = ChatPromptTemplate.from_template(\n",
        "            \"Extract the following information from the given text:\\n{format_instructions}\\n\\nText: {text}\"\n",
        "        )\n",
        "        self.chain = (\n",
        "            {\n",
        "                \"text\": RunnablePassthrough(),\n",
        "                \"format_instructions\": lambda _: output_parser.get_format_instructions()\n",
        "            }\n",
        "            | prompt_template\n",
        "            | self.llm\n",
        "            | output_parser\n",
        "        )\n",
        "        logger.info(\"Chain created\")\n",
        "\n",
        "        # Create a dedicated prompt template for image relevance evaluation\n",
        "        self.image_prompt_template = ChatPromptTemplate.from_template(\n",
        "            \"Website context: {website_context}\\n\"\n",
        "            \"Image data (truncated): {truncated_base64}\\n\"\n",
        "            \"Based on the above information, determine if the image is relevant for the website. \"\n",
        "            \"Answer with 'relevant' or 'irrelevant'.\"\n",
        "        )\n",
        "\n",
        "    def check_image_relevance(self, img_url, website_context):\n",
        "        \"\"\"\n",
        "        Evaluate the relevance of an image for a given website context using the centralized LangChain LLM.\n",
        "\n",
        "        This method converts the image to base64, truncates the encoded string, formats a prompt using a dedicated\n",
        "        prompt template, and then sends the prompt via the shared ChatOpenAI instance.\n",
        "\n",
        "        Args:\n",
        "            img_url (str): The URL of the image to evaluate.\n",
        "            website_context (str): A text description of the website context in which the image will be used.\n",
        "\n",
        "        Returns:\n",
        "            str: The LLM's response, typically \"relevant\" or \"irrelevant\". Returns an error message if conversion fails or an exception occurs.\n",
        "        \"\"\"\n",
        "        base64_img = image_to_base64(img_url)\n",
        "        if not base64_img:\n",
        "            return \"Image conversion failed\"\n",
        "\n",
        "        truncated_base64 = base64_img[:100] + \"...\"\n",
        "        prompt = self.image_prompt_template.format(\n",
        "            website_context=website_context,\n",
        "            truncated_base64=truncated_base64\n",
        "        )\n",
        "        try:\n",
        "          response = self.llm([{\"role\": \"user\", \"content\": prompt}])\n",
        "          # Extract message content from the LLM response, handling various response types\n",
        "          if isinstance(response, list) and len(response) > 0 and hasattr(response[0], \"content\"):\n",
        "              result = response[0].content\n",
        "          elif hasattr(response, \"content\"):\n",
        "              result = response.content\n",
        "          else:\n",
        "              result = str(response)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(\"Error during image relevance check for image %s: %s\", img_url, e)\n",
        "            return \"Relevance check failed\"\n",
        "\n",
        "    def test_sheet_access(self, spreadsheet_id):\n",
        "        try:\n",
        "            sheet = self.service.spreadsheets()\n",
        "            result = sheet.get(spreadsheetId=spreadsheet_id).execute()\n",
        "            logger.info(\"Successfully accessed sheet: %s\", result['properties']['title'])\n",
        "            logger.info(\"Available sheets:\")\n",
        "            for sheet in result['sheets']:\n",
        "                logger.info(\" - %s\", sheet['properties']['title'])\n",
        "        except Exception as e:\n",
        "            logger.error(\"Error accessing spreadsheet: %s\", str(e))\n",
        "\n",
        "    def get_used_range(self, spreadsheet_id, sheet_name):\n",
        "        try:\n",
        "            sheet = self.service.spreadsheets()\n",
        "            result = sheet.get(spreadsheetId=spreadsheet_id, ranges=[sheet_name], includeGridData=False).execute()\n",
        "            grid_properties = result['sheets'][0]['properties']['gridProperties']\n",
        "            row_count = grid_properties['rowCount']\n",
        "            column_count = grid_properties['columnCount']\n",
        "            return f\"{sheet_name}!A1:{chr(64+column_count)}{row_count}\"\n",
        "        except Exception as e:\n",
        "            logger.error(\"Error getting used range: %s\", str(e))\n",
        "            return f\"{sheet_name}!A1:Z1000\"\n",
        "\n",
        "    def read_urls_from_sheet(self, spreadsheet_id, range_name):\n",
        "        logger.info(\"Reading URLs from sheet: %s, range: %s\", spreadsheet_id, range_name)\n",
        "        sheet = self.service.spreadsheets()\n",
        "        try:\n",
        "            result = sheet.values().get(spreadsheetId=spreadsheet_id, range=range_name).execute()\n",
        "            values = result.get('values', [])\n",
        "            logger.debug(\"Raw data from sheet: %s\", values)\n",
        "            if not values:\n",
        "                logger.info(\"No data found in the sheet.\")\n",
        "                return pd.DataFrame()\n",
        "            if values[0][0].lower() == 'urls':\n",
        "                df = pd.DataFrame(values[1:], columns=values[0])\n",
        "            else:\n",
        "                df = pd.DataFrame(values, columns=['Urls'])\n",
        "            logger.info(\"Read %s URLs from the sheet\", len(df))\n",
        "            for url in df['Urls']:\n",
        "                logger.debug(\" - %s\", url)\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            logger.error(\"Error reading from Google Sheets: %s\", str(e))\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def write_data_to_sheet(self, spreadsheet_id, range_name, data):\n",
        "        logger.info(\"Writing data to sheet: %s, range: %s\", spreadsheet_id, range_name)\n",
        "        sheet = self.service.spreadsheets()\n",
        "        try:\n",
        "            sheet_name, start_cell = range_name.split('!')\n",
        "        except ValueError:\n",
        "            logger.error(\"Error: Invalid range format '%s'. Expected format 'SheetName!Cell'.\", range_name)\n",
        "            return\n",
        "        logger.info(\"Clearing existing data in sheet: %s\", sheet_name)\n",
        "        sheet.values().clear(spreadsheetId=spreadsheet_id, range=sheet_name).execute()\n",
        "        end_column = chr(ord('A') + len(data[0]) - 1)\n",
        "        end_row = len(data)\n",
        "        updated_range = f\"{sheet_name}!{start_cell}:{end_column}{end_row}\"\n",
        "        logger.debug(\"Calculated updated range: %s\", updated_range)\n",
        "        body = {'values': data}\n",
        "        try:\n",
        "            result = sheet.values().update(\n",
        "                spreadsheetId=spreadsheet_id, range=updated_range,\n",
        "                valueInputOption='RAW', body=body).execute()\n",
        "            logger.info(\"%s cells updated.\", result.get('updatedCells'))\n",
        "            logger.info(\"Data written: %s rows, %s columns\", len(data), len(data[0]))\n",
        "        except Exception as e:\n",
        "            logger.error(\"Error writing to Google Sheets: %s\", str(e))\n",
        "\n",
        "    def process_url(self, url):\n",
        "        logger.info(\"Processing URL: %s\", url)\n",
        "        webpage_text = fetch_webpage_content(url)\n",
        "        text_splitter = CharacterTextSplitter(chunk_size=4000, chunk_overlap=0)\n",
        "        texts = text_splitter.split_text(webpage_text)\n",
        "        img_tags, div_bg_tags, div_style_tags = scrapemyurl(url)\n",
        "        image_urls = extract_image_urls(img_tags, url)\n",
        "        image_urls += extract_image_urls(div_bg_tags, url)\n",
        "        image_urls += extract_image_urls(div_style_tags, url)\n",
        "        domain_name = get_domain_name(url)\n",
        "        parent_folder_id = '1etOraaUSWfBcPAniYNw6Asbs1hIsJNb6'\n",
        "        drive_folder_id = create_folder_in_drive(domain_name, parent_folder_id)\n",
        "        # --- NEW FEATURE: Check image relevance before saving ---\n",
        "        website_context = \"This is a real estate website showcasing properties for sale and rent.\"\n",
        "        for img_url in image_urls:\n",
        "            if is_real_image(img_url) and is_image_content_type(img_url) and has_sufficient_size(img_url):\n",
        "                relevance = self.check_image_relevance(img_url, website_context)\n",
        "                logger.info(\"Image relevance for %s: %s\", img_url, relevance)\n",
        "                if \"relevant\" in relevance.lower():\n",
        "                    save_image_to_drive(img_url, drive_folder_id)\n",
        "                else:\n",
        "                    logger.info(\"Skipping image %s due to irrelevance.\", img_url)\n",
        "            else:\n",
        "                logger.info(\"Invalid image URL: %s\", img_url)\n",
        "        # ---------------------------------------------------------\n",
        "        try:\n",
        "            parsed_response = self.chain.invoke(texts[0])\n",
        "            logger.info(\"URL processed successfully: %s\", url)\n",
        "            return {schema.name: parsed_response.get(schema.name, '') for schema in self.response_schemas}\n",
        "        except Exception as e:\n",
        "            logger.error(\"Error processing URL %s: %s\", url, str(e))\n",
        "            return {schema.name: '' for schema in self.response_schemas}\n",
        "\n",
        "    def process_urls(self, spreadsheet_id, input_range, output_range):\n",
        "        logger.info(\"Starting URL processing\")\n",
        "        urls_df = self.read_urls_from_sheet(spreadsheet_id, input_range)\n",
        "        if urls_df.empty:\n",
        "            logger.info(\"No URLs to process. Exiting.\")\n",
        "            return\n",
        "        data = []\n",
        "        for index, url in enumerate(urls_df['Urls'], 1):\n",
        "            logger.info(\"Processing URL %s/%s: %s\", index, len(urls_df), url)\n",
        "            parsed_info = self.process_url(url)\n",
        "            row_data = [url] + [parsed_info.get(schema.name, '') for schema in self.response_schemas]\n",
        "            data.append(row_data)\n",
        "            logger.debug(\"Processed data for %s:\", url)\n",
        "            for schema, value in zip(self.response_schemas, row_data[1:]):\n",
        "                if value is None:\n",
        "                    logger.debug(\" - %s: None\", schema.name)\n",
        "                elif isinstance(value, str) and len(value) > 50:\n",
        "                    logger.debug(\" - %s: %s...\", schema.name, value[:50])\n",
        "                else:\n",
        "                    logger.debug(\" - %s: %s\", schema.name, value)\n",
        "        headers = ['URL'] + [schema.name for schema in self.response_schemas]\n",
        "        self.write_data_to_sheet(spreadsheet_id, output_range, [headers] + data)\n",
        "        logger.info(\"URL processing completed\")\n",
        "\n",
        "    def run(self):\n",
        "        spreadsheet_id = '1kBQgpQhzEaxL1R-FQtGVTSIgkLqZScyPrbaZ_mKewvo'\n",
        "        input_sheet_name = 'urls_to_scrape'\n",
        "        output_sheet_name = 'offres_immo'\n",
        "        logger.info(\"Testing sheet access for spreadsheet ID: %s\", spreadsheet_id)\n",
        "        self.test_sheet_access(spreadsheet_id)\n",
        "        input_range = self.get_used_range(spreadsheet_id, input_sheet_name)\n",
        "        output_range = f\"{output_sheet_name}!A1\"\n",
        "        logger.info(\"Starting main process with spreadsheet ID: %s\", spreadsheet_id)\n",
        "        logger.info(\"Input range: %s\", input_range)\n",
        "        logger.info(\"Output range: %s\", output_range)\n",
        "        self.process_urls(spreadsheet_id, input_range, output_range)\n",
        "        logger.info(\"Main process completed\")\n"
      ],
      "metadata": {
        "id": "o4Ep9L_twSzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processor = OffresImmoProcessor()\n",
        "processor.run()"
      ],
      "metadata": {
        "id": "QGumbYMaKine"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "pip install pylint radon pytest"
      ],
      "metadata": {
        "id": "xsNWczY5P93C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sGht0hy9x4yP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pylint --enable=design offres_immos.py"
      ],
      "metadata": {
        "id": "DuXxNMRSQDPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!radon cc offres_immos.py -a"
      ],
      "metadata": {
        "id": "AOnAUXiiQyFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test_offres_immos.py\n",
        "\n",
        "import pytest\n",
        "import logging\n",
        "from offres_immos import (\n",
        "    is_real_image,\n",
        "    extract_background_image_url,\n",
        "    get_domain_name,\n",
        "    fetch_webpage_content,\n",
        "    scrapemyurl,\n",
        "    extract_image_urls,\n",
        "    has_sufficient_size\n",
        ")\n",
        "import requests\n",
        "from unittest.mock import patch, MagicMock\n",
        "from bs4 import BeautifulSoup\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "\n",
        "# Setup logger for testing\n",
        "logger = logging.getLogger(\"TestLogger\")\n",
        "logger.setLevel(logging.DEBUG)\n",
        "if not logger.handlers:\n",
        "    ch = logging.StreamHandler()\n",
        "    ch.setLevel(logging.DEBUG)\n",
        "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "    ch.setFormatter(formatter)\n",
        "    logger.addHandler(ch)\n",
        "\n",
        "def test_is_real_image_valid():\n",
        "    \"\"\"\n",
        "    Test that is_real_image returns True for a valid image URL.\n",
        "    \"\"\"\n",
        "    result = is_real_image(\"https://example.com/image.jpg\")\n",
        "    assert result is True\n",
        "    logger.info(\"test_is_real_image_valid passed: expected True and got %s\", result)\n",
        "\n",
        "def test_is_real_image_invalid():\n",
        "    \"\"\"\n",
        "    Test that is_real_image returns False for an image URL containing excluded keywords.\n",
        "    \"\"\"\n",
        "    result = is_real_image(\"https://example.com/logo.png\")\n",
        "    assert result is False\n",
        "    logger.info(\"test_is_real_image_invalid passed: expected False and got %s\", result)\n",
        "\n",
        "def test_extract_background_image_url():\n",
        "    \"\"\"\n",
        "    Test that extract_background_image_url correctly extracts a URL from a style string.\n",
        "    \"\"\"\n",
        "    style = \"background-image: url('https://example.com/bg.jpg');\"\n",
        "    result = extract_background_image_url(style)\n",
        "    assert result == \"https://example.com/bg.jpg\"\n",
        "    logger.info(\"test_extract_background_image_url passed: extracted URL is %s\", result)\n",
        "\n",
        "def test_extract_background_image_url_no_url():\n",
        "    \"\"\"\n",
        "    Test that extract_background_image_url returns None if no URL is present in the style.\n",
        "    \"\"\"\n",
        "    style = \"background-color: red;\"\n",
        "    result = extract_background_image_url(style)\n",
        "    assert result is None\n",
        "    logger.info(\"test_extract_background_image_url_no_url passed: expected None and got %s\", result)\n",
        "\n",
        "def test_get_domain_name():\n",
        "    \"\"\"\n",
        "    Test that get_domain_name correctly extracts the domain from a URL.\n",
        "    \"\"\"\n",
        "    url = \"http://sub.example.com/path\"\n",
        "    result = get_domain_name(url)\n",
        "    assert result == \"sub.example.com\"\n",
        "    logger.info(\"test_get_domain_name passed: extracted domain is %s\", result)\n",
        "\n",
        "@patch('offres_immos.requests.Session.get')\n",
        "def test_fetch_webpage_content_success(mock_get):\n",
        "    \"\"\"\n",
        "    Test that fetch_webpage_content successfully fetches and parses content from a webpage.\n",
        "    \"\"\"\n",
        "    sample_html = \"<html><body><div id='content'><p>Hello World!</p></div></body></html>\"\n",
        "    mock_response = MagicMock()\n",
        "    mock_response.status_code = 200\n",
        "    mock_response.content = sample_html.encode('utf-8')\n",
        "    mock_get.return_value = mock_response\n",
        "\n",
        "    content = fetch_webpage_content(\"http://example.com\")\n",
        "    # The cleaning process in fetch_webpage_content removes punctuation, so \"Hello World\" is expected.\n",
        "    assert \"Hello World\" in content\n",
        "    logger.info(\"test_fetch_webpage_content_success passed: 'Hello World' found in content\")\n",
        "\n",
        "\n",
        "@patch('offres_immos.requests.get')\n",
        "def test_scrapemyurl(mock_get):\n",
        "    \"\"\"\n",
        "    Test that scrapemyurl extracts image tags, divs with data-bg,\n",
        "    and divs with background-image style correctly.\n",
        "    \"\"\"\n",
        "    sample_html = \"<html><body><img src='image1.jpg'/><div data-bg='bg_image.jpg'></div><div style='background-image: url(\\\"style_image.jpg\\\");'></div></body></html>\"\n",
        "    mock_response = MagicMock()\n",
        "    mock_response.status_code = 200\n",
        "    mock_response.content = sample_html.encode('utf-8')\n",
        "    mock_get.return_value = mock_response\n",
        "\n",
        "    img_tags, div_bg_tags, div_style_tags = scrapemyurl(\"http://example.com\")\n",
        "    assert len(img_tags) == 1\n",
        "    assert len(div_bg_tags) == 1\n",
        "    assert len(div_style_tags) == 1\n",
        "    logger.info(\"test_scrapemyurl passed: found %d img tags, %d divs with data-bg, and %d divs with style\",\n",
        "                len(img_tags), len(div_bg_tags), len(div_style_tags))\n",
        "\n",
        "\n",
        "\n",
        "def test_extract_image_urls():\n",
        "    \"\"\"\n",
        "    Test that extract_image_urls correctly constructs full image URLs from img tags and divs with data-bg.\n",
        "    \"\"\"\n",
        "    base_url = \"http://example.com/\"\n",
        "    html = \"\"\"\n",
        "    <html>\n",
        "      <body>\n",
        "        <img src=\"image1.jpg\" />\n",
        "        <div data-bg=\"bg_image.jpg\"></div>\n",
        "      </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    img_tags = soup.find_all('img')\n",
        "    div_bg_tags = soup.find_all('div', attrs={'data-bg': True})\n",
        "\n",
        "    image_urls = extract_image_urls(img_tags, base_url)\n",
        "    image_urls += extract_image_urls(div_bg_tags, base_url)\n",
        "\n",
        "    assert \"http://example.com/image1.jpg\" in image_urls\n",
        "    assert \"http://example.com/bg_image.jpg\" in image_urls\n",
        "    logger.info(\"test_extract_image_urls passed: extracted URLs are %s\", image_urls)\n",
        "\n",
        "@patch('offres_immos.requests.get')\n",
        "def test_has_sufficient_size(mock_get):\n",
        "    \"\"\"\n",
        "    Test that has_sufficient_size returns True for a dummy image with dimensions above the threshold.\n",
        "    \"\"\"\n",
        "    # Create a dummy image (150x150 pixels)\n",
        "    image = Image.new('RGB', (150, 150), color='red')\n",
        "    buf = BytesIO()\n",
        "    image.save(buf, format='JPEG')\n",
        "    buf.seek(0)\n",
        "    fake_image_content = buf.read()\n",
        "\n",
        "    mock_response = MagicMock()\n",
        "    mock_response.status_code = 200\n",
        "    mock_response.content = fake_image_content\n",
        "    mock_get.return_value = mock_response\n",
        "\n",
        "    result = has_sufficient_size(\"http://example.com/dummy.jpg\")\n",
        "    assert result is True\n",
        "    logger.info(\"test_has_sufficient_size passed: image size sufficient, returned %s\", result)\n",
        "\n"
      ],
      "metadata": {
        "id": "L_OLXJOKTiym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest --maxfail=1 --disable-warnings -q"
      ],
      "metadata": {
        "id": "HZRvE_0reWHs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}